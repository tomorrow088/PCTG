"""
æ•°æ®é›†æ¨¡å—
Adversarial Camouflage Dataset for SINet Attack

æ•°æ®é›†ç»“æ„:
data/
â”œâ”€â”€ images/          # åŸå§‹å›¾åƒ
â”œâ”€â”€ masks/           # ç›®æ ‡æ©ç  (å¯é€‰)
â”œâ”€â”€ backgrounds/     # èƒŒæ™¯å›¾åƒ (å¯é€‰)
â””â”€â”€ annotations/     # æ ‡æ³¨æ–‡ä»¶ (å¯é€‰)

æ”¯æŒçš„æ•°æ®æ ¼å¼:
- å›¾åƒ: .jpg, .png, .jpeg
- æ©ç : .png (äºŒå€¼å›¾åƒ)
- æ ‡æ³¨: .json, .xml
"""

import torch
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import torchvision.transforms.functional as TF

import os
import cv2
import numpy as np
from PIL import Image, ImageDraw
import json
import random
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union
import albumentations as A
from albumentations.pytorch import ToTensorV2


class AdversarialCamouflageDataset(Dataset):
    """
    å¯¹æŠ—æ€§è¿·å½©æ•°æ®é›†
    
    æ”¯æŒå¤šç§æ•°æ®æº:
    1. äººç‰©å›¾åƒ + è‡ªåŠ¨ç”Ÿæˆæ©ç 
    2. äººç‰©å›¾åƒ + æ‰‹åŠ¨æ ‡æ³¨æ©ç   
    3. é€šç”¨ç›®æ ‡æ£€æµ‹æ•°æ®é›†
    """
    
    def __init__(self, 
                 data_dir: str,
                 split: str = 'train',
                 transforms: Optional[A.Compose] = None,
                 mask_mode: str = 'auto',  # 'auto', 'provided', 'bbox'
                 target_size: Tuple[int, int] = (256, 256),
                 debug: bool = False):
        """
        Args:
            data_dir: æ•°æ®é›†æ ¹ç›®å½•
            split: æ•°æ®é›†åˆ†å‰² ('train', 'val', 'test')
            transforms: æ•°æ®å˜æ¢
            mask_mode: æ©ç æ¨¡å¼
            target_size: ç›®æ ‡å›¾åƒå°ºå¯¸
            debug: è°ƒè¯•æ¨¡å¼ (ä½¿ç”¨å°‘é‡æ•°æ®)
        """
        self.data_dir = Path(data_dir)
        self.split = split
        self.transforms = transforms
        self.mask_mode = mask_mode
        self.target_size = target_size
        self.debug = debug
        
        # æ•°æ®è·¯å¾„
        self.images_dir = self.data_dir / 'images' / split
        self.masks_dir = self.data_dir / 'masks' / split
        self.annotations_dir = self.data_dir / 'annotations' / split
        
        # åŠ è½½æ•°æ®åˆ—è¡¨
        self.data_list = self._load_data_list()
        
        # è°ƒè¯•æ¨¡å¼ - é™åˆ¶æ•°æ®é‡
        if debug:
            self.data_list = self.data_list[:min(100, len(self.data_list))]
        
        print(f"âœ… {split}æ•°æ®é›†åŠ è½½å®Œæˆ: {len(self.data_list)}å¼ å›¾åƒ")
        print(f"   æ•°æ®ç›®å½•: {self.data_dir}")
        print(f"   æ©ç æ¨¡å¼: {mask_mode}")
        print(f"   å›¾åƒå°ºå¯¸: {target_size}")
    
    def _load_data_list(self) -> List[Dict]:
        """åŠ è½½æ•°æ®åˆ—è¡¨"""
        data_list = []
        
        if not self.images_dir.exists():
            # å¦‚æœæ²¡æœ‰åˆ†ç¦»çš„train/valç›®å½•ï¼Œä½¿ç”¨æ ¹ç›®å½•
            self.images_dir = self.data_dir / 'images'
            self.masks_dir = self.data_dir / 'masks'
            self.annotations_dir = self.data_dir / 'annotations'
        
        if not self.images_dir.exists():
            print(f"âš ï¸ å›¾åƒç›®å½•ä¸å­˜åœ¨: {self.images_dir}")
            print("ğŸ“ åˆ›å»ºç¤ºä¾‹æ•°æ®ç»“æ„...")
            self._create_dummy_data()
            return self._load_data_list()
        
        # æ”¯æŒçš„å›¾åƒæ ¼å¼
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}
        
        # éå†å›¾åƒæ–‡ä»¶
        for image_path in self.images_dir.iterdir():
            if image_path.suffix.lower() in image_extensions:
                
                # æ„å»ºå¯¹åº”çš„æ©ç å’Œæ ‡æ³¨è·¯å¾„
                mask_path = self.masks_dir / f"{image_path.stem}.png"
                annotation_path = self.annotations_dir / f"{image_path.stem}.json"
                
                data_item = {
                    'image_path': image_path,
                    'mask_path': mask_path if mask_path.exists() else None,
                    'annotation_path': annotation_path if annotation_path.exists() else None,
                    'image_id': image_path.stem
                }
                
                data_list.append(data_item)
        
        return data_list
    
    def _create_dummy_data(self):
        """åˆ›å»ºç¤ºä¾‹æ•°æ® (è°ƒè¯•ç”¨)"""
        print("ğŸ­ åˆ›å»ºç¤ºä¾‹æ•°æ®...")
        
        # åˆ›å»ºç›®å½•
        for dir_path in [self.images_dir, self.masks_dir, self.annotations_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆç¤ºä¾‹å›¾åƒ
        for i in range(10):
            # åˆ›å»ºéšæœºå›¾åƒ
            image = np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)
            image_pil = Image.fromarray(image)
            image_path = self.images_dir / f"sample_{i:03d}.jpg"
            image_pil.save(image_path)
            
            # åˆ›å»ºéšæœºæ©ç 
            mask = np.zeros((256, 256), dtype=np.uint8)
            # åœ¨ä¸­å¿ƒåŒºåŸŸåˆ›å»ºä¸€ä¸ªæ¤­åœ†æ©ç 
            cv2.ellipse(mask, (128, 128), (60, 40), 0, 0, 360, 255, -1)
            mask_pil = Image.fromarray(mask)
            mask_path = self.masks_dir / f"sample_{i:03d}.png"
            mask_pil.save(mask_path)
            
            # åˆ›å»ºç¤ºä¾‹æ ‡æ³¨
            annotation = {
                'image_id': f"sample_{i:03d}",
                'objects': [{
                    'class': 'person',
                    'bbox': [68, 88, 128, 128],  # [x, y, w, h]
                    'segmentation': [[68, 88, 196, 88, 196, 216, 68, 216]]
                }]
            }
            
            annotation_path = self.annotations_dir / f"sample_{i:03d}.json"
            with open(annotation_path, 'w') as f:
                json.dump(annotation, f)
        
        print(f"   åˆ›å»ºäº†{10}ä¸ªç¤ºä¾‹æ•°æ®æ–‡ä»¶")
    
    def __len__(self) -> int:
        return len(self.data_list)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """è·å–æ•°æ®é¡¹"""
        data_item = self.data_list[idx]
        
        # åŠ è½½å›¾åƒ
        image = self._load_image(data_item['image_path'])
        
        # åŠ è½½æˆ–ç”Ÿæˆæ©ç 
        mask = self._load_or_generate_mask(data_item, image.shape[:2])
        
        # åº”ç”¨å˜æ¢
        if self.transforms is not None:
            transformed = self.transforms(image=image, mask=mask)
            image = transformed['image']
            mask = transformed['mask']
        
        # ç¡®ä¿æ©ç æ˜¯æ­£ç¡®çš„å½¢çŠ¶
        if len(mask.shape) == 2:
            mask = mask.unsqueeze(0)  # [H, W] -> [1, H, W]
        elif len(mask.shape) == 3 and mask.shape[0] == 3:
            mask = mask[0:1]  # å–ç¬¬ä¸€ä¸ªé€šé“
        
        # æ©ç äºŒå€¼åŒ–
        mask = (mask > 0.5).float()
        
        return {
            'image': image,
            'mask': mask,
            'image_id': data_item['image_id'],
            'image_path': str(data_item['image_path'])
        }
    
    def _load_image(self, image_path: Path) -> np.ndarray:
        """åŠ è½½å›¾åƒ"""
        image = cv2.imread(str(image_path))
        if image is None:
            raise ValueError(f"æ— æ³•åŠ è½½å›¾åƒ: {image_path}")
        
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # è°ƒæ•´å°ºå¯¸
        image = cv2.resize(image, self.target_size, interpolation=cv2.INTER_LINEAR)
        
        return image
    
    def _load_or_generate_mask(self, data_item: Dict, image_shape: Tuple[int, int]) -> np.ndarray:
        """åŠ è½½æˆ–ç”Ÿæˆæ©ç """
        
        if self.mask_mode == 'provided' and data_item['mask_path'] is not None:
            # åŠ è½½æä¾›çš„æ©ç 
            mask = cv2.imread(str(data_item['mask_path']), cv2.IMREAD_GRAYSCALE)
            if mask is not None:
                mask = cv2.resize(mask, self.target_size, interpolation=cv2.INTER_NEAREST)
                return mask
        
        elif self.mask_mode == 'bbox' and data_item['annotation_path'] is not None:
            # ä»è¾¹ç•Œæ¡†ç”Ÿæˆæ©ç 
            mask = self._generate_mask_from_bbox(data_item['annotation_path'])
            if mask is not None:
                return mask
        
        # è‡ªåŠ¨ç”Ÿæˆæ©ç  (é»˜è®¤)
        return self._generate_automatic_mask(image_shape)
    
    def _generate_mask_from_bbox(self, annotation_path: Path) -> Optional[np.ndarray]:
        """ä»è¾¹ç•Œæ¡†æ ‡æ³¨ç”Ÿæˆæ©ç """
        try:
            with open(annotation_path, 'r') as f:
                annotation = json.load(f)
            
            mask = np.zeros(self.target_size[::-1], dtype=np.uint8)  # (H, W)
            
            for obj in annotation.get('objects', []):
                if obj.get('class') in ['person', 'human', 'people']:
                    bbox = obj['bbox']  # [x, y, w, h]
                    x, y, w, h = bbox
                    
                    # è°ƒæ•´è¾¹ç•Œæ¡†åˆ°ç›®æ ‡å°ºå¯¸
                    scale_x = self.target_size[0] / annotation.get('image_width', self.target_size[0])
                    scale_y = self.target_size[1] / annotation.get('image_height', self.target_size[1])
                    
                    x = int(x * scale_x)
                    y = int(y * scale_y)
                    w = int(w * scale_x)
                    h = int(h * scale_y)
                    
                    # å¡«å……çŸ©å½¢æ©ç 
                    mask[y:y+h, x:x+w] = 255
            
            return mask
            
        except Exception as e:
            print(f"âš ï¸ åŠ è½½æ ‡æ³¨å¤±è´¥: {e}")
            return None
    
    def _generate_automatic_mask(self, image_shape: Tuple[int, int]) -> np.ndarray:
        """è‡ªåŠ¨ç”Ÿæˆæ©ç  (ä¸­å¿ƒæ¤­åœ†åŒºåŸŸ)"""
        mask = np.zeros(self.target_size[::-1], dtype=np.uint8)  # (H, W)
        
        # åœ¨å›¾åƒä¸­å¿ƒç”Ÿæˆæ¤­åœ†æ©ç 
        center_x, center_y = self.target_size[0] // 2, self.target_size[1] // 2
        
        # æ¤­åœ†å‚æ•° (éšæœºåŒ–å¢åŠ å¤šæ ·æ€§)
        radius_x = random.randint(30, 80)
        radius_y = random.randint(40, 100)
        angle = random.randint(-30, 30)
        
        cv2.ellipse(
            mask, 
            (center_x, center_y), 
            (radius_x, radius_y), 
            angle, 0, 360, 255, -1
        )
        
        return mask


def get_train_transforms(image_size: int = 256, augmentation: bool = True) -> A.Compose:
    """è·å–è®­ç»ƒæ—¶çš„æ•°æ®å˜æ¢"""
    
    transforms_list = [
        A.Resize(image_size, image_size),
    ]
    
    if augmentation:
        # å‡ ä½•å˜æ¢
        transforms_list.extend([
            A.HorizontalFlip(p=0.5),
            A.Rotate(limit=15, p=0.3),
            A.RandomResizedCrop(
                height=image_size, 
                width=image_size, 
                scale=(0.8, 1.0),
                ratio=(0.8, 1.2),
                p=0.3
            ),
        ])
        
        # é¢œè‰²å˜æ¢
        transforms_list.extend([
            A.ColorJitter(
                brightness=0.2,
                contrast=0.2, 
                saturation=0.2,
                hue=0.1,
                p=0.5
            ),
            A.RandomBrightnessContrast(
                brightness_limit=0.2,
                contrast_limit=0.2,
                p=0.3
            ),
        ])
        
        # å™ªå£°å’Œæ¨¡ç³Š
        transforms_list.extend([
            A.GaussNoise(var_limit=(10, 50), p=0.2),
            A.GaussianBlur(blur_limit=(3, 7), p=0.2),
        ])
    
    # å½’ä¸€åŒ–å’Œå¼ é‡è½¬æ¢
    transforms_list.extend([
        A.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225],
        ),
        ToTensorV2(),
    ])
    
    return A.Compose(
        transforms_list,
        additional_targets={'mask': 'mask'}
    )


def get_val_transforms(image_size: int = 256) -> A.Compose:
    """è·å–éªŒè¯æ—¶çš„æ•°æ®å˜æ¢"""
    return A.Compose([
        A.Resize(image_size, image_size),
        A.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225],
        ),
        ToTensorV2(),
    ], additional_targets={'mask': 'mask'})


def create_data_loaders(data_dir: str, 
                       config: Dict,
                       num_workers: int = 4,
                       debug: bool = False) -> Tuple[DataLoader, DataLoader]:
    """åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨"""
    
    # è·å–å˜æ¢
    train_transforms = get_train_transforms(
        image_size=config.get('image_size', 256),
        augmentation=True
    )
    
    val_transforms = get_val_transforms(
        image_size=config.get('image_size', 256)
    )
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = AdversarialCamouflageDataset(
        data_dir=data_dir,
        split='train',
        transforms=train_transforms,
        debug=debug
    )
    
    val_dataset = AdversarialCamouflageDataset(
        data_dir=data_dir,
        split='val', 
        transforms=val_transforms,
        debug=debug
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.get('batch_size', 4),
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.get('val_batch_size', 8),
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=False
    ) if len(val_dataset) > 0 else None
    
    return train_loader, val_loader


class VisualizationUtils:
    """å¯è§†åŒ–å·¥å…·"""
    
    @staticmethod
    def visualize_batch(batch: Dict[str, torch.Tensor], 
                       predictions: Optional[Dict[str, torch.Tensor]] = None,
                       save_path: Optional[str] = None):
        """å¯è§†åŒ–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®"""
        import matplotlib.pyplot as plt
        
        images = batch['image']
        masks = batch['mask']
        batch_size = min(4, images.shape[0])  # æœ€å¤šæ˜¾ç¤º4å¼ å›¾
        
        # åå½’ä¸€åŒ–
        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
        images_denorm = images * std + mean
        images_denorm = torch.clamp(images_denorm, 0, 1)
        
        # åˆ›å»ºå­å›¾
        cols = 3 if predictions is None else 4
        fig, axes = plt.subplots(batch_size, cols, figsize=(cols*3, batch_size*3))
        
        if batch_size == 1:
            axes = axes.reshape(1, -1)
        
        for i in range(batch_size):
            # åŸå›¾
            axes[i, 0].imshow(images_denorm[i].permute(1, 2, 0))
            axes[i, 0].set_title('Original Image')
            axes[i, 0].axis('off')
            
            # æ©ç 
            axes[i, 1].imshow(masks[i, 0], cmap='gray')
            axes[i, 1].set_title('Target Mask')
            axes[i, 1].axis('off')
            
            # æ©ç å åŠ 
            overlay = images_denorm[i].permute(1, 2, 0).clone()
            mask_colored = torch.zeros_like(overlay)
            mask_colored[:, :, 0] = masks[i, 0]  # çº¢è‰²é€šé“
            overlay = 0.7 * overlay + 0.3 * mask_colored
            
            axes[i, 2].imshow(overlay)
            axes[i, 2].set_title('Mask Overlay')
            axes[i, 2].axis('off')
            
            # ç”Ÿæˆç»“æœ (å¦‚æœæœ‰)
            if predictions is not None and 'generated_image' in predictions:
                gen_image = predictions['generated_image'][i]
                gen_image_denorm = gen_image * std + mean
                gen_image_denorm = torch.clamp(gen_image_denorm, 0, 1)
                
                axes[i, 3].imshow(gen_image_denorm.permute(1, 2, 0))
                axes[i, 3].set_title('Generated Image')
                axes[i, 3].axis('off')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            plt.close()
        else:
            plt.show()


def test_dataset():
    """æµ‹è¯•æ•°æ®é›†"""
    print("ğŸ§ª æµ‹è¯•æ•°æ®é›†...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    dataset = AdversarialCamouflageDataset(
        data_dir='./data',
        split='train',
        transforms=get_train_transforms(),
        debug=True
    )
    
    print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
    
    # æµ‹è¯•æ•°æ®åŠ è½½
    sample = dataset[0]
    print(f"å›¾åƒå½¢çŠ¶: {sample['image'].shape}")
    print(f"æ©ç å½¢çŠ¶: {sample['mask'].shape}")
    print(f"å›¾åƒID: {sample['image_id']}")
    
    # æµ‹è¯•æ•°æ®åŠ è½½å™¨
    loader = DataLoader(dataset, batch_size=2, shuffle=True)
    batch = next(iter(loader))
    
    print(f"æ‰¹æ¬¡å›¾åƒå½¢çŠ¶: {batch['image'].shape}")
    print(f"æ‰¹æ¬¡æ©ç å½¢çŠ¶: {batch['mask'].shape}")
    
    # å¯è§†åŒ–
    VisualizationUtils.visualize_batch(batch, save_path='test_batch.png')
    print("âœ… æ•°æ®é›†æµ‹è¯•å®Œæˆï¼Œå¯è§†åŒ–ç»“æœä¿å­˜ä¸º test_batch.png")


if __name__ == "__main__":
    test_dataset()
