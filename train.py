"""
ä¸»è®­ç»ƒè„šæœ¬
Adversarial Camouflage Generation with SINet

ä½¿ç”¨æ–¹æ³•:
    python train.py --config config/default.yaml
    python train.py --resume checkpoints/latest.pth
    python train.py --debug  # è°ƒè¯•æ¨¡å¼
"""

import argparse
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import torch
import torch.distributed as dist
from torch.utils.data import DataLoader, DistributedSampler
import yaml
import random
import numpy as np
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# é¡¹ç›®æ¨¡å—
from trainer import AdversarialTrainer, create_trainer_from_config
from model_config import get_default_config, get_debug_config, get_paper_config
from data.dataset import AdversarialCamouflageDataset
from data.transforms import get_train_transforms, get_val_transforms


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Adversarial Camouflage Training')
    
    # åŸºç¡€å‚æ•°
    parser.add_argument('--config', type=str, default=None,
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--resume', type=str, default=None,
                       help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--data_dir', type=str, default='./data',
                       help='æ•°æ®é›†æ ¹ç›®å½•')
    
    # è®­ç»ƒæ¨¡å¼
    parser.add_argument('--debug', action='store_true',
                       help='è°ƒè¯•æ¨¡å¼ (å°æ•°æ®é›†)')
    parser.add_argument('--paper', action='store_true',
                       help='è®ºæ–‡å®éªŒæ¨¡å¼ (å®Œæ•´è®­ç»ƒ)')
    
    # è®¾å¤‡è®¾ç½®
    parser.add_argument('--device', type=str, default='auto',
                       help='è®­ç»ƒè®¾å¤‡ (auto/cuda/cpu)')
    parser.add_argument('--num_gpus', type=int, default=-1,
                       help='GPUæ•°é‡ (-1=è‡ªåŠ¨æ£€æµ‹)')
    
    # åˆ†å¸ƒå¼è®­ç»ƒ
    parser.add_argument('--distributed', action='store_true',
                       help='åˆ†å¸ƒå¼è®­ç»ƒ')
    parser.add_argument('--local_rank', type=int, default=0,
                       help='æœ¬åœ°rank (åˆ†å¸ƒå¼è®­ç»ƒ)')
    
    # æ•°æ®åŠ è½½
    parser.add_argument('--num_workers', type=int, default=4,
                       help='æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°')
    parser.add_argument('--pin_memory', action='store_true', default=True,
                       help='å›ºå®šå†…å­˜')
    
    # å®éªŒè®¾ç½®
    parser.add_argument('--experiment_name', type=str, default=None,
                       help='å®éªŒåç§°')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­')
    
    return parser.parse_args()


def setup_distributed_training(args):
    """è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒ"""
    if args.distributed:
        # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
        dist.init_process_group(backend='nccl')
        torch.cuda.set_device(args.local_rank)
        
        print(f"ğŸŒ åˆ†å¸ƒå¼è®­ç»ƒåˆå§‹åŒ–å®Œæˆ")
        print(f"   Rank: {dist.get_rank()}/{dist.get_world_size()}")
        print(f"   Local Rank: {args.local_rank}")


def setup_device(args):
    """è®¾ç½®è®­ç»ƒè®¾å¤‡"""
    if args.device == 'auto':
        if torch.cuda.is_available():
            device = 'cuda'
            if args.num_gpus == -1:
                args.num_gpus = torch.cuda.device_count()
        else:
            device = 'cpu'
            args.num_gpus = 0
    else:
        device = args.device
    
    if args.distributed:
        device = f'cuda:{args.local_rank}'
    
    print(f"ğŸ’» è®¾å¤‡è®¾ç½®:")
    print(f"   ä¸»è®¾å¤‡: {device}")
    print(f"   GPUæ•°é‡: {args.num_gpus}")
    print(f"   CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    return device


def set_random_seed(seed):
    """è®¾ç½®éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    
    # ç¡®ä¿ç»“æœå¯å¤ç°
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    print(f"ğŸ² éšæœºç§å­è®¾ç½®: {seed}")


def load_config(args):
    """åŠ è½½é…ç½®"""
    if args.config is not None:
        # ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®
        with open(args.config, 'r') as f:
            config = yaml.safe_load(f)
        print(f"ğŸ“„ ä»æ–‡ä»¶åŠ è½½é…ç½®: {args.config}")
    
    elif args.debug:
        config = get_debug_config()
        print("ğŸ› ä½¿ç”¨è°ƒè¯•é…ç½®")
    
    elif args.paper:
        config = get_paper_config()
        print("ğŸ“ ä½¿ç”¨è®ºæ–‡å®éªŒé…ç½®")
    
    else:
        config = get_default_config()
        print("âš™ï¸ ä½¿ç”¨é»˜è®¤é…ç½®")
    
    # ä»å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
    if args.experiment_name is not None:
        config['experiment_name'] = args.experiment_name
    
    return config


def create_datasets(args, config):
    """åˆ›å»ºæ•°æ®é›†"""
    print("ğŸ“Š åˆ›å»ºæ•°æ®é›†...")
    
    # è®­ç»ƒå˜æ¢
    train_transforms = get_train_transforms(
        image_size=config['training'].get('image_size', 256),
        augmentation=True
    )
    
    # éªŒè¯å˜æ¢
    val_transforms = get_val_transforms(
        image_size=config['training'].get('image_size', 256)
    )
    
    # è®­ç»ƒæ•°æ®é›†
    train_dataset = AdversarialCamouflageDataset(
        data_dir=args.data_dir,
        split='train',
        transforms=train_transforms,
        debug=args.debug
    )
    
    # éªŒè¯æ•°æ®é›†
    val_dataset = AdversarialCamouflageDataset(
        data_dir=args.data_dir,
        split='val',
        transforms=val_transforms,
        debug=args.debug
    )
    
    print(f"   è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}")
    print(f"   éªŒè¯æ ·æœ¬æ•°: {len(val_dataset)}")
    
    return train_dataset, val_dataset


def create_data_loaders(train_dataset, val_dataset, config, args):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    print("ğŸ”„ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    
    # åˆ†å¸ƒå¼é‡‡æ ·å™¨
    train_sampler = None
    val_sampler = None
    
    if args.distributed:
        train_sampler = DistributedSampler(train_dataset)
        val_sampler = DistributedSampler(val_dataset, shuffle=False)
    
    # è®­ç»ƒæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['training'].batch_size,
        shuffle=(train_sampler is None),
        sampler=train_sampler,
        num_workers=args.num_workers,
        pin_memory=args.pin_memory,
        drop_last=True
    )
    
    # éªŒè¯æ•°æ®åŠ è½½å™¨
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['training'].val_batch_size,
        shuffle=False,
        sampler=val_sampler,
        num_workers=args.num_workers,
        pin_memory=args.pin_memory,
        drop_last=False
    ) if len(val_dataset) > 0 else None
    
    print(f"   è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
    print(f"   éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader) if val_loader else 0}")
    
    return train_loader, val_loader


def create_trainer(config, device, args):
    """åˆ›å»ºè®­ç»ƒå™¨"""
    print("ğŸ—ï¸ åˆ›å»ºè®­ç»ƒå™¨...")
    
    trainer = AdversarialTrainer(
        config=config,
        device=device,
        distributed=args.distributed
    )
    
    return trainer


def resume_training(trainer, resume_path):
    """æ¢å¤è®­ç»ƒ"""
    print(f"ğŸ”„ æ¢å¤è®­ç»ƒ: {resume_path}")
    
    checkpoint = torch.load(resume_path, map_location=trainer.device)
    
    # åŠ è½½æ¨¡å‹çŠ¶æ€
    trainer.generator.load_state_dict(checkpoint['model_state_dict'])
    trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    if 'scheduler_state_dict' in checkpoint and trainer.scheduler is not None:
        trainer.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    
    # æ¢å¤è®­ç»ƒçŠ¶æ€
    trainer.current_epoch = checkpoint['epoch'] + 1
    trainer.best_metric = checkpoint.get('best_metric', 0.0)
    
    print(f"   æ¢å¤åˆ°epoch {trainer.current_epoch}")
    print(f"   æœ€ä½³æŒ‡æ ‡: {trainer.best_metric:.3f}")


def print_training_info(config, train_loader, val_loader):
    """æ‰“å°è®­ç»ƒä¿¡æ¯"""
    print("\n" + "="*60)
    print("ğŸš€ è®­ç»ƒé…ç½®æ€»è§ˆ")
    print("="*60)
    
    print(f"ğŸ“‹ æ¨¡å‹é…ç½®:")
    print(f"   ç”Ÿæˆå™¨: PCTG ({config['pctg'].encoder_name})")
    print(f"   æ£€æµ‹å™¨: SINet + CLIP")
    print(f"   ç‰©ç†çº¦æŸ: {config['pctg'].printable_colors_only}")
    
    print(f"\nğŸ¯ è®­ç»ƒå‚æ•°:")
    print(f"   æ€»è½®æ•°: {config['training'].epochs}")
    print(f"   æ‰¹æ¬¡å¤§å°: {config['training'].batch_size}")
    print(f"   å­¦ä¹ ç‡: {config['training'].learning_rate}")
    print(f"   ä¼˜åŒ–å™¨: {config['training'].optimizer}")
    
    print(f"\nğŸ“Š æ•°æ®ä¿¡æ¯:")
    print(f"   è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}")
    print(f"   éªŒè¯æ‰¹æ¬¡: {len(val_loader) if val_loader else 0}")
    
    print(f"\nğŸ’¾ æŸå¤±æƒé‡:")
    print(f"   å¯¹æŠ—æŸå¤±: {config['training'].adversarial_weight}")
    print(f"   å†…å®¹æŸå¤±: {config['training'].content_weight}")
    print(f"   æ„ŸçŸ¥æŸå¤±: {config['training'].perceptual_weight}")
    print(f"   çº¹ç†æŸå¤±: {config['training'].texture_weight}")
    print(f"   ç‰©ç†çº¦æŸ: {config['training'].physical_constraint_weight}")
    
    print("="*60 + "\n")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ­ Adversarial Camouflage Training with SINet")
    print("="*60)
    
    # è§£æå‚æ•°
    args = parse_arguments()
    
    # è®¾ç½®éšæœºç§å­
    set_random_seed(args.seed)
    
    # è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒ
    setup_distributed_training(args)
    
    # è®¾ç½®è®¾å¤‡
    device = setup_device(args)
    
    # åŠ è½½é…ç½®
    config = load_config(args)
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset, val_dataset = create_datasets(args, config)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader = create_data_loaders(
        train_dataset, val_dataset, config, args
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = create_trainer(config, device, args)
    
    # æ¢å¤è®­ç»ƒ (å¦‚æœéœ€è¦)
    if args.resume is not None:
        resume_training(trainer, args.resume)
    
    # æ‰“å°è®­ç»ƒä¿¡æ¯
    print_training_info(config, train_loader, val_loader)
    
    try:
        # å¼€å§‹è®­ç»ƒ
        trainer.train(train_loader, val_loader)
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        # ä¿å­˜å½“å‰çŠ¶æ€
        checkpoint_path = trainer.checkpoint_dir / "interrupted_checkpoint.pth"
        trainer._save_checkpoint({}, {})
        print(f"ğŸ’¾ å·²ä¿å­˜ä¸­æ–­æ£€æŸ¥ç‚¹: {checkpoint_path}")
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        # æ¸…ç†èµ„æº
        if args.distributed:
            dist.destroy_process_group()
        
        print("\nğŸ è®­ç»ƒè„šæœ¬ç»“æŸ")


if __name__ == "__main__":
    main()
